{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  One_hot_encoding for ordinal and categorial columns (rank information on education and poverty is lo\n",
    "2.  Check for NAs \n",
    "    -> delete employment columns, health insurance\n",
    "    -> delete rows with no information on recommandation of doctor (poor answering behavior) \n",
    "    -> take rows of opinion columns and impute NAs with k nearest neighbour \n",
    "3.  Delete the same rows in the labels dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess raw data\n",
    "# split raw data into train and eval\n",
    "# log_reg fit \n",
    "# preprocess test data \n",
    "# log_reg fit\n",
    "# submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent.parent / \"data\" / \"raw\"\n",
    "raw_df = pd.read_csv(data_path / \"training_set_features.csv\", index_col=\"respondent_id\")\n",
    "labels_df = pd.read_csv(data_path / \"training_set_labels.csv\", index_col=\"respondent_id\")\n",
    "test_df = pd.read_csv(data_path / \"test_set_features.csv\", index_col=\"respondent_id\")\n",
    "\n",
    "all_df = raw_df.join(labels_df)\n",
    "#all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-enconding without employment_occuptation (is employment_occuptation either dropped or Weight of Evidence Encoded)\n",
    "    # drop column employment_occupation \n",
    "    # identify columns with categorial data \n",
    "    # just use columns with nominal data\n",
    "    # getdummies() -> drop first column to dismiss multicollinarity, keep NaN column\n",
    "raw_df = raw_df.drop(columns = ['employment_occupation'])\n",
    "\n",
    "non_numerical_obj = raw_df.columns[raw_df.dtypes == \"object\"]\n",
    "non_numerical_obj\n",
    "\n",
    "nominal_list = ['race', 'sex', \n",
    "       'marital_status', 'rent_or_own', 'employment_status', 'hhs_geo_region',\n",
    "       'census_msa', 'employment_industry']\n",
    "\n",
    "raw_df_encoded = pd.get_dummies(raw_df, columns=nominal_list, drop_first = True, dummy_na=True)\n",
    "test_df_encoded = pd.get_dummies(test_df, columns=nominal_list, drop_first = True, dummy_na=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of Evidence Encoding of Occuptation on raw_data\n",
    "# 'h1n1_vaccine', 'seasonal_vaccine' \n",
    "def WOE(var, tar):\n",
    "    all_df[var] = all_df[var].fillna('NoData')\n",
    "    k = all_df[[var,tar]].groupby(var)[tar].agg(['count','sum']).reset_index()\n",
    "    k.columns = [var,'Count','Good']\n",
    "    k['Bad'] = k['Count'] - k['Good']\n",
    "    k['Good %'] = (k['Good'] / k['Good'].sum()*100).round(2)\n",
    "    k['Bad %'] = (k['Bad'] / k['Bad'].sum()*100).round(2)\n",
    "    k[var+tar+'_WOE'] = np.log(k['Good %'] / k['Bad %']).round(2)\n",
    "    k = k.sort_values(by=var+tar+'_WOE')\n",
    "    return(k)\n",
    "h1n1_WOE = WOE('employment_occupation' , 'h1n1_vaccine')\n",
    "seasonal_WOE = WOE('employment_occupation' , 'seasonal_vaccine')\n",
    "\n",
    "WOE_df_season = pd.merge(all_df[['seasonal_vaccine','employment_occupation']],seasonal_WOE[['employment_occupation','employment_occupationseasonal_vaccine_WOE']],\n",
    "     left_on='employment_occupation',\n",
    "     right_on='employment_occupation',how='left')\n",
    "WOE_df_h1n1 = pd.merge(all_df[['h1n1_vaccine','employment_occupation']],h1n1_WOE[['employment_occupation','employment_occupationh1n1_vaccine_WOE']],\n",
    "     left_on='employment_occupation',\n",
    "     right_on='employment_occupation',how='left')\n",
    "\n",
    "\n",
    "WOE_df_both = WOE_df_h1n1.join(WOE_df_season, lsuffix='_h1n1', rsuffix='_seasonal')\n",
    "WOE_mean = WOE_df_both[['employment_occupationh1n1_vaccine_WOE', \n",
    "                           'employment_occupationseasonal_vaccine_WOE']].mean(axis = 1)\n",
    "\n",
    "raw_df_encoded['employment_occupation_WOE'] = WOE_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight of Evidence Encoding of Occuptation on test_data\n",
    "# 'h1n1_vaccine', 'seasonal_vaccine' \n",
    "test_df_encoded['employment_occupation_new'] =test_df_encoded['employment_occupation'].replace(np.nan, 'NoData')\n",
    "\n",
    "test_WOE_df_season = pd.merge(test_df_encoded['employment_occupation_new'],seasonal_WOE[['employment_occupation','employment_occupationseasonal_vaccine_WOE']],\n",
    "     left_on='employment_occupation_new',\n",
    "     right_on='employment_occupation',how='left')\n",
    "test_WOE_df_season.index =  test_df_encoded.index\n",
    "\n",
    "test_WOE_df_h1n1 = pd.merge(test_df_encoded[['employment_occupation_new']],h1n1_WOE[['employment_occupation','employment_occupationh1n1_vaccine_WOE']],\n",
    "     left_on='employment_occupation_new',\n",
    "     right_on='employment_occupation',how='left')\n",
    "test_WOE_df_h1n1.index =  test_df_encoded.index\n",
    "\n",
    "\n",
    "test_WOE_df_both = test_WOE_df_h1n1 .join(test_WOE_df_season, lsuffix='_h1n1', rsuffix='_seasonal')\n",
    "\n",
    "test_WOE_df_both.index =  test_df_encoded.index\n",
    "\n",
    "\n",
    "test_df_encoded['employment_occupation_WOE'] = test_WOE_df_both[['employment_occupationh1n1_vaccine_WOE', \n",
    "                           'employment_occupationseasonal_vaccine_WOE']].mean(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "test_df_encoded = test_df_encoded.drop(columns = ['employment_occupation','employment_occupation_new' ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding of ordinal data \n",
    "    # identifiying ordinal data columns \n",
    "    # change NaN in ordinal data columns to most frequent value \n",
    "    # change strings in ordinal data to numbers \n",
    "    # delete values in columns wich where imputed with most frequent values\n",
    "    # impute NaN values with KNN\n",
    "    \n",
    "ordinal_list = ['age_group','education', 'income_poverty' ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute misssing values in ordinal data columns with most frequent value \n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "raw_df_encoded_mode = imp_mode.fit_transform(raw_df_encoded[ordinal_list])\n",
    "raw_df_encoded_mode_pd = pd.DataFrame(raw_df_encoded_mode, index = raw_df_encoded.index)\n",
    "\n",
    "raw_df_encoded['age_group'] = raw_df_encoded_mode_pd[0]\n",
    "raw_df_encoded['education'] = raw_df_encoded_mode_pd[1]\n",
    "raw_df_encoded['income_poverty'] = raw_df_encoded_mode_pd[2]\n",
    "\n",
    "test_imp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "test_df_encoded_mode = test_imp_mode.fit_transform(test_df_encoded[ordinal_list])\n",
    "test_df_encoded_mode_pd = pd.DataFrame(test_df_encoded_mode, index = test_df_encoded.index )\n",
    "\n",
    "test_df_encoded['age_group'] = test_df_encoded_mode_pd[0]\n",
    "test_df_encoded['education'] = test_df_encoded_mode_pd[1]\n",
    "test_df_encoded['income_poverty'] = test_df_encoded_mode_pd[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change strings in ordinal data to numbers \n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_enc = OrdinalEncoder ( \n",
    "    categories = [\n",
    "        ['18 - 34 Years', '35 - 44 Years', '45 - 54 Years', '55 - 64 Years', '65+ Years'],\n",
    "        ['< 12 Years', '12 Years', 'Some College', 'College Graduate' ],\n",
    "        ['Below Poverty', '<= $75,000, Above Poverty', '> $75,000'],\n",
    "     ]\n",
    ")\n",
    "\n",
    "raw_df_encoded_ordinal = ordinal_enc.fit_transform(raw_df_encoded[ordinal_list])\n",
    "raw_df_encoded_ordinal = pd.DataFrame(raw_df_encoded_ordinal, raw_df_encoded.index)\n",
    "#ordinal_enc.categories_\n",
    "\n",
    "raw_df_encoded['age_group'] = raw_df_encoded_ordinal[0]\n",
    "raw_df_encoded['education'] = raw_df_encoded_ordinal[1]\n",
    "raw_df_encoded['income_poverty'] = raw_df_encoded_ordinal[2]\n",
    "\n",
    "test_df_encoded_ordinal = ordinal_enc.fit_transform(test_df_encoded[ordinal_list])\n",
    "test_df_encoded_ordinal = pd.DataFrame(test_df_encoded_ordinal, test_df_encoded.index)\n",
    "\n",
    "\n",
    "test_df_encoded['age_group'] = test_df_encoded_ordinal[0]\n",
    "test_df_encoded['education'] = test_df_encoded_ordinal[1]\n",
    "test_df_encoded['income_poverty'] = test_df_encoded_ordinal[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete values in columns wich where imputed with most frequent values\n",
    "\n",
    "education_encoded = raw_df_encoded['education'].copy()\n",
    "education_na = raw_df['education'].isna()\n",
    "education_encoded[education_na] = np.nan\n",
    "\n",
    "raw_df_encoded['education'] = education_encoded\n",
    "\n",
    "income_poverty_encoded = raw_df_encoded['income_poverty'].copy()\n",
    "income_poverty_na = raw_df['income_poverty'].isna()\n",
    "income_poverty_encoded[income_poverty_na] = np.nan\n",
    "\n",
    "raw_df_encoded['income_poverty'] = income_poverty_encoded\n",
    "\n",
    "test_education_encoded = test_df_encoded['education'].copy()\n",
    "test_education_na = test_df['education'].isna()\n",
    "test_education_encoded[test_education_na] = np.nan\n",
    "\n",
    "test_df_encoded['education'] = test_education_encoded\n",
    "\n",
    "test_income_poverty_encoded = test_df_encoded['income_poverty'].copy()\n",
    "test_income_poverty_na = test_df['income_poverty'].isna()\n",
    "test_income_poverty_encoded[test_income_poverty_na] = np.nan\n",
    "\n",
    "test_df_encoded['income_poverty'] = test_income_poverty_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##caculate KNN for the whole dataset  // either this or next cell\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#from sklearn.impute import KNNImputer\n",
    "#imputer = KNNImputer(\n",
    "#    missing_values=np.nan,\n",
    "#    n_neighbors=5,\n",
    "#    weights=\"distance\",\n",
    "#    metric=\"nan_euclidean\",\n",
    "#    copy=True,\n",
    "#    add_indicator=False,\n",
    "#)\n",
    "\n",
    "#imputer.fit(raw_df_encoded)\n",
    "\n",
    "#knn_array = imputer.transform(raw_df_encoded)\n",
    "\n",
    "#raw_df_encoded_imputed = pd.DataFrame(knn_array, raw_df_encoded.index)\n",
    "#raw_df_encoded_imputed.columns = raw_df_encoded.columns\n",
    "\n",
    "#change KNN imputations in ordinal data columns to \"categories\"\n",
    "\n",
    "#raw_df_encoded_imputed['education'].round()\n",
    "#raw_df_encoded_imputed['income_poverty'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternativly to KNN impute missing values with NaN\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mode = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "raw_df_encoded_imputed = imp_mode.fit_transform(raw_df_encoded)\n",
    "\n",
    "raw_df_encoded_imputed = pd.DataFrame(raw_df_encoded_imputed, columns = raw_df_encoded.columns, index = raw_df_encoded.index)\n",
    "\n",
    "raw_df_median = imp_mode.fit(raw_df_encoded)\n",
    "test_df_encoded_imputed = imp_mode.transform(test_df_encoded)\n",
    "\n",
    "test_df_encoded_imputed = pd.DataFrame(test_df_encoded_imputed, columns = test_df_encoded.columns, index = test_df_encoded.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming\n",
    "raw_df_encoded_imputed_list = raw_df_encoded_imputed.columns.tolist()\n",
    "\n",
    "renamed_list = []\n",
    "for column_name in raw_df_encoded_imputed_list:\n",
    "    column_name = re.sub(pattern=r\"\\$(\\d\\d)....\", repl=\"\\\\1k\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\<\\=|\\<\\s\", repl=\"less_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\>\\=|\\>\\s\", repl=\"gr_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"(\\w)\\,\\s(\\w)\", repl=\"\\\\1_\\\\2\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\s\\-\\s\", repl=\"_-_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"(\\w)\\s+(\\w)\", repl=\"\\\\1_\\\\2\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"(\\d\\+)\\s\", repl=\"\\\\1_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\_\\_\", repl=\"_\", string=column_name)\n",
    "    renamed_list.append(column_name)\n",
    "    \n",
    "raw_df_encoded_imputed.columns = renamed_list\n",
    "test_df_encoded_imputed.columns = renamed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path_processed = Path.cwd().parent.parent / \"data\" / \"processed\"\n",
    "#raw_df_encoded.to_csv(data_path_processed / \"training_set_features_1hot_na.csv\",)\n",
    "#test_df_encoded.to_csv(data_path_processed / \"test_set_features_1hot_na.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "marital_status_nan                    employment_status_nan                   0.876134\n",
       "employment_status_nan                 marital_status_nan                      0.876134\n",
       "employment_status_Not_in_Labor_Force  employment_industry_nan                 0.789401\n",
       "employment_industry_nan               employment_status_Not_in_Labor_Force    0.789401\n",
       "rent_or_own_nan                       employment_status_nan                   0.770415\n",
       "employment_status_nan                 rent_or_own_nan                         0.770415\n",
       "rent_or_own_nan                       marital_status_nan                      0.742348\n",
       "marital_status_nan                    rent_or_own_nan                         0.742348\n",
       "health_worker                         employment_industry_fcxhlnwr            0.694224\n",
       "employment_industry_fcxhlnwr          health_worker                           0.694224\n",
       "doctor_recc_seasonal                  doctor_recc_h1n1                        0.591868\n",
       "doctor_recc_h1n1                      doctor_recc_seasonal                    0.591868\n",
       "behavioral_outside_home               behavioral_large_gatherings             0.582328\n",
       "behavioral_large_gatherings           behavioral_outside_home                 0.582328\n",
       "opinion_seas_risk                     opinion_h1n1_risk                       0.562724\n",
       "opinion_h1n1_risk                     opinion_seas_risk                       0.562724\n",
       "race_White                            race_Hispanic                           0.521664\n",
       "race_Hispanic                         race_White                              0.521664\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for multi collinarity \n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "corr_data = raw_df_encoded_imputed.corr().abs()\n",
    "sorted_corr_data = corr_data.unstack().sort_values(ascending=False)\n",
    "ones = corr_data.unstack().sort_values(ascending=False) != 1.0\n",
    "without_ones = sorted_corr_data[ones]\n",
    "NaNs = corr_data.unstack().sort_values(ascending=False).notna()\n",
    "\n",
    "without_ones_and_Na = without_ones[NaNs]\n",
    "big_corrs = corr_data.unstack().sort_values(ascending=False) > 0.5\n",
    "without_ones_and_Na[big_corrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employment_status_Not in Labor Force    26707\n",
       "marital_status_Not Married              26707\n",
       "census_msa_MSA, Principle City          26707\n",
       "race_Other or Multiple                  26707\n",
       "household_children                          0\n",
       "opinion_seas_sick_from_vacc                 0\n",
       "age_group                                   0\n",
       "education                                   0\n",
       "income_poverty                              0\n",
       "household_adults                            0\n",
       "race_Hispanic                               0\n",
       "opinion_seas_vacc_effective                 0\n",
       "race_White                                  0\n",
       "race_nan                                    0\n",
       "sex_Male                                    0\n",
       "sex_nan                                     0\n",
       "marital_status_nan                          0\n",
       "opinion_seas_risk                           0\n",
       "opinion_h1n1_sick_from_vacc                 0\n",
       "rent_or_own_nan                             0\n",
       "opinion_h1n1_risk                           0\n",
       "h1n1_knowledge                              0\n",
       "behavioral_antiviral_meds                   0\n",
       "behavioral_avoidance                        0\n",
       "behavioral_face_mask                        0\n",
       "behavioral_wash_hands                       0\n",
       "behavioral_large_gatherings                 0\n",
       "behavioral_outside_home                     0\n",
       "behavioral_touch_face                       0\n",
       "doctor_recc_h1n1                            0\n",
       "doctor_recc_seasonal                        0\n",
       "chronic_med_condition                       0\n",
       "child_under_6_months                        0\n",
       "health_worker                               0\n",
       "health_insurance                            0\n",
       "opinion_h1n1_vacc_effective                 0\n",
       "rent_or_own_Rent                            0\n",
       "employment_occupation_WOE                   0\n",
       "employment_industry_nan                     0\n",
       "employment_status_Unemployed                0\n",
       "employment_industry_ldnlellj                0\n",
       "employment_industry_mcubkhph                0\n",
       "employment_industry_mfikgejo                0\n",
       "employment_industry_msuufmds                0\n",
       "employment_industry_nduyfdeo                0\n",
       "employment_industry_phxvnwax                0\n",
       "employment_industry_pxcmvdjn                0\n",
       "employment_industry_qnlwzans                0\n",
       "employment_industry_rucpziij                0\n",
       "employment_industry_saaquncn                0\n",
       "employment_industry_vjjrobsf                0\n",
       "employment_industry_wlfvacwt                0\n",
       "employment_industry_wxleyezf                0\n",
       "employment_industry_xicduogh                0\n",
       "employment_industry_xqicxuve                0\n",
       "employment_industry_haxffmxo                0\n",
       "employment_industry_fcxhlnwr                0\n",
       "employment_industry_dotnnunm                0\n",
       "hhs_geo_region_lzgpxyit                     0\n",
       "employment_status_nan                       0\n",
       "hhs_geo_region_bhuqouqj                     0\n",
       "hhs_geo_region_dqpwygqj                     0\n",
       "hhs_geo_region_fpwskwrf                     0\n",
       "hhs_geo_region_kbazzjca                     0\n",
       "hhs_geo_region_lrircsnp                     0\n",
       "hhs_geo_region_mlyzmhmf                     0\n",
       "employment_industry_cfqqtusy                0\n",
       "hhs_geo_region_oxchjgsf                     0\n",
       "hhs_geo_region_qufhixun                     0\n",
       "hhs_geo_region_nan                          0\n",
       "census_msa_Non-MSA                          0\n",
       "census_msa_nan                              0\n",
       "employment_industry_atmlpfrs                0\n",
       "h1n1_concern                                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pd_df_encoded_imputed_stand.isna().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "raw_df_encoded_imputed_stand = scaler.fit_transform(raw_df_encoded_imputed)\n",
    "raw_df_encoded_imputed_stand = pd.DataFrame(raw_df_encoded_imputed_stand, columns = raw_df_encoded_imputed.columns, index = raw_df_encoded_imputed.index)\n",
    "\n",
    "test_df_encoded_imputed_stand = scaler.fit_transform(test_df_encoded_imputed)\n",
    "test_df_encoded_imputed_stand = pd.DataFrame(test_df_encoded_imputed_stand, columns = test_df_encoded_imputed.columns, index = test_df_encoded_imputed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_preprocessed = Path.cwd().parent.parent / \"data\" / \"processed\"\n",
    "\n",
    "raw_pd_df_encoded_imputed_stand = pd.DataFrame(raw_df_encoded_imputed_stand, columns = raw_df_encoded.columns, index = raw_df_encoded.index)\n",
    "raw_df_encoded_imputed_stand.to_csv (data_path_preprocessed / 'training_set_features_Preprocessing_nominal_ordinal_WOE_Impute_Stand.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_preprocessed = Path.cwd().parent.parent / \"data\" / \"processed\"\n",
    "\n",
    "#test_pd_df_encoded_imputed_stand = pd.DataFrame(test_df_encoded_imputed_stand, columns = test_df_encoded.columns, index = test_df_encoded.index)\n",
    "test_df_encoded_imputed_stand.to_csv (data_path_preprocessed / 'test_set_features_Preprocessing_nominal_ordinal_WOE_Impute_Stand.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37-ds]",
   "language": "python",
   "name": "conda-env-py37-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
