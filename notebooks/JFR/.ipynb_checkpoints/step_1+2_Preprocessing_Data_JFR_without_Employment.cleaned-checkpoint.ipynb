{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  One_hot_encoding for ordinal and categorial columns (rank information on education and poverty is lo\n",
    "2.  Check for NAs \n",
    "    -> delete employment columns, health insurance\n",
    "    -> delete rows with no information on recommandation of doctor (poor answering behavior) \n",
    "    -> take rows of opinion columns and impute NAs with k nearest neighbour \n",
    "3.  Delete the same rows in the labels dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent.parent / \"data\" / \"raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/johannes/Nextcloud/Eigene Dateien/04 LEARN/Python/drivendata.flu-shot-learning/notebooks/data/raw/training_set_features.csv does not exist: '/Users/johannes/Nextcloud/Eigene Dateien/04 LEARN/Python/drivendata.flu-shot-learning/notebooks/data/raw/training_set_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ab32a694f9e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"training_set_features.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"respondent_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/py37-ds/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37-ds/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37-ds/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37-ds/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37-ds/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /Users/johannes/Nextcloud/Eigene Dateien/04 LEARN/Python/drivendata.flu-shot-learning/notebooks/data/raw/training_set_features.csv does not exist: '/Users/johannes/Nextcloud/Eigene Dateien/04 LEARN/Python/drivendata.flu-shot-learning/notebooks/data/raw/training_set_features.csv'"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(data_path / \"training_set_features.csv\", index_col=\"respondent_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(data_path / \"test_set_features.csv\", index_col=\"respondent_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-enconding without employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numerical_obj = raw_df.columns[raw_df.dtypes == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numerical_list = non_numerical_obj.tolist()[0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded = pd.get_dummies(raw_df, columns=non_numerical_list, dummy_na=True)\n",
    "test_df_encoded = pd.get_dummies(test_df, columns=non_numerical_list, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_column_list = test_df_encoded.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in test_column_list:\n",
    "    column_name = re.sub(pattern=r\"\\$(\\d\\d)....\", repl=\"\\\\1k\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\<\\=|\\<\\s\", repl=\"less_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\>\\=|\\>\\s\", repl=\"gr_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"(\\w)\\,\\s(\\w)\", repl=\"\\\\1_\\\\2\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\s\\-\\s\", repl=\"_-_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"(\\w)\\s+(\\w)\", repl=\"\\\\1_\\\\2\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"(\\d\\+)\\s\", repl=\"\\\\1_\", string=column_name)\n",
    "    column_name = re.sub(pattern=r\"\\_\\_\", repl=\"_\", string=column_name)\n",
    "    renamed_list.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded.columns = renamed_list\n",
    "test_df_encoded.columns = renamed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_processed = Path.cwd().parent / \"data\" / \"processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded.to_csv(data_path_processed / \"training_set_features_1hot_na.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_encoded.to_csv(data_path_processed / \"test_set_features_1hot_na.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for columns with high number of Nas\n",
    "\n",
    "empty_df_1 = pd.DataFrame()\n",
    "empty_df_1[\"column_name\"] = raw_df_encoded.columns\n",
    "nulls_list = []\n",
    "\n",
    "for column in raw_df_encoded.columns:\n",
    "    nulls = raw_df_encoded[column].isnull().sum()\n",
    "    nulls_list.append(nulls)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "empty_df_1[\"no_Na\"] = nulls_list\n",
    "\n",
    "empty_df_1.sort_values(by=\"no_Na\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop columns with very high number of NAs\n",
    "raw_df_encoded_naDrop = raw_df_encoded.drop(\n",
    "    [\"employment_industry\", \"employment_occupation\", \"health_insurance\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df_2 = pd.DataFrame()\n",
    "empty_df_2[\"column_name\"] = raw_df_encoded_naDrop.columns\n",
    "nulls_list = []\n",
    "\n",
    "for column in raw_df_encoded_naDrop.columns:\n",
    "    nulls = raw_df_encoded_naDrop[column].isnull().sum()\n",
    "    nulls_list.append(nulls)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "empty_df_2[\"no_Na\"] = nulls_list\n",
    "\n",
    "empty_df_2.sort_values(by=\"no_Na\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check if those who didnt answer doctor recommandation, answered in other questions -> they didn't\n",
    "\n",
    "no_recc = raw_df_encoded_naDrop[raw_df_encoded_naDrop[\"doctor_recc_h1n1\"].isna()]\n",
    "\n",
    "empty_df_3 = pd.DataFrame()\n",
    "empty_df_3[\"column_name\"] = no_recc.columns\n",
    "nulls_list = []\n",
    "\n",
    "for column in no_recc.columns:\n",
    "    nulls = no_recc[column].isnull().sum()\n",
    "    nulls_list.append(nulls)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "empty_df_3[\"no_Na\"] = nulls_list\n",
    "\n",
    "empty_df_4 = empty_df_3.join(empty_df_2, lsuffix=\"_3\", rsuffix=\"_2\")\n",
    "\n",
    "empty_df_4.sort_values(by=\"no_Na_3\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping respondents with NA at doc_recc\n",
    "\n",
    "raw_df_encoded_naDrop_2 = raw_df_encoded_naDrop.dropna(\n",
    "    axis=0, subset=[\"doctor_recc_h1n1\", \"doctor_recc_seasonal\"]\n",
    ")\n",
    "len(raw_df_encoded_naDrop_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for further NAs\n",
    "\n",
    "empty_df_5 = pd.DataFrame()\n",
    "empty_df_5[\"column_name\"] = raw_df_encoded_naDrop_2.columns\n",
    "nulls_list = []\n",
    "binary_list = []\n",
    "\n",
    "for column in raw_df_encoded_naDrop_2.columns:\n",
    "    nulls = raw_df_encoded_naDrop_2[column].isnull().sum()\n",
    "    binary = len(raw_df_encoded_naDrop_2[column].value_counts())\n",
    "\n",
    "    nulls_list.append(nulls)\n",
    "    binary_list.append(binary)\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "empty_df_5[\"no_Na\"] = nulls_list\n",
    "empty_df_5[\"binary\"] = binary_list\n",
    "\n",
    "empty_df_5.sort_values(by=\"no_Na\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100, \"display.max_columns\", 20)\n",
    "raw_df_encoded_naDrop_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(\n",
    "    missing_values=np.nan,\n",
    "    n_neighbors=3,\n",
    "    weights=\"distance\",\n",
    "    metric=\"nan_euclidean\",\n",
    "    copy=True,\n",
    "    add_indicator=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded_naDrop_KNN = imputer.fit_transform(raw_df_encoded_naDrop_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded_naDrop_KNN_pd = pd.DataFrame(\n",
    "    data=raw_df_encoded_naDrop_KNN, columns=raw_df_encoded_naDrop_2.columns, index = raw_df_encoded_naDrop_2.index\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 20)\n",
    "\n",
    "raw_df_encoded_naDrop_KNN_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df_6 = pd.DataFrame()\n",
    "empty_df_6[\"column_name\"] = raw_df_encoded_naDrop_KNN_pd.columns\n",
    "nulls_list = []\n",
    "binary_list = []\n",
    "\n",
    "for column in raw_df_encoded_naDrop_KNN_pd.columns:\n",
    "    nulls = raw_df_encoded_naDrop_KNN_pd[column].isnull().sum()\n",
    "    binary = len(raw_df_encoded_naDrop_KNN_pd[column].value_counts())\n",
    "\n",
    "    nulls_list.append(nulls)\n",
    "    binary_list.append(binary)\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "empty_df_6[\"no_Na\"] = nulls_list\n",
    "empty_df_6[\"binary\"] = binary_list\n",
    "\n",
    "empty_df_6.sort_values(by=\"binary\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded_naDrop_KNN_pd.index.name = 'respondent_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_encoded_naDrop_KNN_pd.to_csv(data_path_processed / \"training_set_features_1hot_na_cleaned.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows in label_data that are not available in processed data due to cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH2 = Path.cwd().parent / \"data\" / \"raw\"\n",
    "labels_df = pd.read_csv(\n",
    "    DATA_PATH2 / \"training_set_labels.csv\", \n",
    "    index_col=\"respondent_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_respondent_ids = raw_df_encoded_naDrop_KNN_pd.index.values\n",
    "final_respondent_ids.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df_cleaned = labels_df.iloc[final_respondent_ids, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df_cleaned.to_csv(data_path_processed / \"training_set_labels_cleaned.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test-Data\n",
    "# check for columns with high number of Nas\n",
    "\n",
    "empty_df_1 = pd.DataFrame()\n",
    "empty_df_1[\"column_name\"] = test_df_encoded.columns\n",
    "nulls_list = []\n",
    "\n",
    "for column in test_df_encoded.columns:\n",
    "    nulls = test_df_encoded[column].isnull().sum()\n",
    "    nulls_list.append(nulls)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "\n",
    "empty_df_1[\"no_Na\"] = nulls_list\n",
    "\n",
    "empty_df_1.sort_values(by=\"no_Na\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_encoded_naDrop = test_df_encoded.drop(\n",
    "    [\"employment_industry\", \"employment_occupation\", \"health_insurance\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_encoded_naDrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping respondents with NA at doc_recc\n",
    "\n",
    "test_df_encoded_naDrop_2 = test_df_encoded_naDrop.dropna(\n",
    "    axis=0, subset=[\"doctor_recc_h1n1\", \"doctor_recc_seasonal\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_encoded_naDrop.to_csv(data_path_processed / \"test_set_features_1hot_cleaned.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_encoded_naDrop2 = test_df_encoded_naDrop.fillna(raw_df_encoded_naDrop_KNN_pd.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_encoded_naDrop2.to_csv(data_path_processed / \"test_set_features_1hot_cleaned_imputed_mean.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37-ds]",
   "language": "python",
   "name": "conda-env-py37-ds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
